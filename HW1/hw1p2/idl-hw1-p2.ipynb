{"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"standard","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HW1: Frame-Level Speech Recognition","metadata":{"id":"F9ERgBpbcMmB"}},{"cell_type":"markdown","source":"In this homework, you will be working with MFCC data consisting of 28 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame.","metadata":{"id":"CLkH6GMGcWcE"}},{"cell_type":"markdown","source":"# Libraries","metadata":{"id":"z4vZbDmJvMp1"}},{"cell_type":"code","source":"!pip install numpy torch torchsummaryX wandb","metadata":{"id":"rwYu9sSUnSho","colab":{"base_uri":"https://localhost:8080/"},"outputId":"884cbeb6-dd3f-4a24-adc6-b3c740eb8f0a","execution":{"iopub.status.busy":"2023-09-23T19:26:50.793252Z","iopub.execute_input":"2023-09-23T19:26:50.793828Z","iopub.status.idle":"2023-09-23T19:27:06.350929Z","shell.execute_reply.started":"2023-09-23T19:26:50.793784Z","shell.execute_reply":"2023-09-23T19:27:06.349149Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.23.5)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.0.0+cpu)\nRequirement already satisfied: torchsummaryX in /opt/conda/lib/python3.10/site-packages (1.3.0)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch) (3.12.2)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.6.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from torchsummaryX) (2.0.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.30.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.0.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.7.22)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->torchsummaryX) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->torchsummaryX) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->torchsummaryX) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torchsummaryX import summary\nimport sklearn\nimport gc\nimport zipfile\nimport pandas as pd\nimport torch.multiprocessing as mp\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nimport time\nimport random\nimport os\nimport datetime\nimport wandb\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nnum_processes = 4\nprint(\"Device: \", device)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qI4qfx7tiBZt","outputId":"355cf503-d7fd-4397-ed18-8edd809e2a4e","execution":{"iopub.status.busy":"2023-09-23T19:27:06.354222Z","iopub.execute_input":"2023-09-23T19:27:06.354622Z","iopub.status.idle":"2023-09-23T19:27:06.364575Z","shell.execute_reply.started":"2023-09-23T19:27:06.354586Z","shell.execute_reply":"2023-09-23T19:27:06.363063Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stdout","text":"Device:  cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"### If you are using colab, you can import google drive to save model checkpoints in a folder\n# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"8yBgXjKV1O0Z","execution":{"iopub.status.busy":"2023-09-23T19:27:06.366776Z","iopub.execute_input":"2023-09-23T19:27:06.367240Z","iopub.status.idle":"2023-09-23T19:27:06.379871Z","shell.execute_reply.started":"2023-09-23T19:27:06.367197Z","shell.execute_reply":"2023-09-23T19:27:06.378486Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"### PHONEME LIST\nPHONEMES = [\n            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']","metadata":{"id":"N-9qE20hmCgQ","execution":{"iopub.status.busy":"2023-09-23T19:27:06.383052Z","iopub.execute_input":"2023-09-23T19:27:06.383403Z","iopub.status.idle":"2023-09-23T19:27:06.393354Z","shell.execute_reply.started":"2023-09-23T19:27:06.383373Z","shell.execute_reply":"2023-09-23T19:27:06.392435Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle","metadata":{"id":"ZIi0Big7vPa9"}},{"cell_type":"markdown","source":"This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully.","metadata":{"id":"BBCbeRhixGM7"}},{"cell_type":"code","source":"# !pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n# !mkdir /root/.kaggle\n# !touch /root/.kaggle/kaggle.json\n# with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n#     f.write('{\"username\":\"suriyaganesh\",\"key\":\"74848730a16663d8aca7f6ebdeacb10f\"}')\n#     # Put your kaggle username & key here\n\n# !chmod 600 /root/.kaggle/kaggle.json","metadata":{"id":"TPBUd7Cnl-Rx","outputId":"ad06720c-057c-47f9-9387-af7ec34fecc1","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-09-23T19:27:06.394740Z","iopub.execute_input":"2023-09-23T19:27:06.395779Z","iopub.status.idle":"2023-09-23T19:27:06.409634Z","shell.execute_reply.started":"2023-09-23T19:27:06.395746Z","shell.execute_reply":"2023-09-23T19:27:06.408341Z"},"trusted":true},"execution_count":136,"outputs":[]},{"cell_type":"code","source":"!ls /kaggle/input/","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:27:06.411174Z","iopub.execute_input":"2023-09-23T19:27:06.411897Z","iopub.status.idle":"2023-09-23T19:27:07.567511Z","shell.execute_reply.started":"2023-09-23T19:27:06.411863Z","shell.execute_reply":"2023-09-23T19:27:07.566179Z"},"trusted":true},"execution_count":137,"outputs":[{"name":"stdout","text":"11785-hw1p2-f23\n","output_type":"stream"}]},{"cell_type":"code","source":"# # commands to download data from kaggle\n\n# !kaggle competitions download -c 11785-hw1p2-f23\n# !mkdir '/content/data'\n\n# !unzip -qo /content/11785-hw1p2-f23.zip -d '/content/data'","metadata":{"id":"if2Somqfbje1","outputId":"c55f975c-4b31-41f9-877c-b1259efbcfad","colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2023-09-23T19:27:07.569430Z","iopub.execute_input":"2023-09-23T19:27:07.570123Z","iopub.status.idle":"2023-09-23T19:27:07.576560Z","shell.execute_reply.started":"2023-09-23T19:27:07.570084Z","shell.execute_reply":"2023-09-23T19:27:07.575326Z"},"trusted":true},"execution_count":138,"outputs":[]},{"cell_type":"markdown","source":"# Dataset","metadata":{"id":"Vuzce0_TdcaR"}},{"cell_type":"markdown","source":"This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n\nBefore running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?","metadata":{"id":"2_7QgMbBdgPp"}},{"cell_type":"code","source":"# from numpy.core.overrides import verify_matching_signatures\n# import numpy as np\n# # Create a list of matrices (replace these with your actual matrices)\n# matrix_list = [np.array([[1, 2], [3, 4]]), np.array([[5, 6], [7, 8]]), np.array([[9, 10], [11, 12]])]\n\n# # Concatenate matrices horizontally (along columns)\n# horizontal_concatenated = np.concatenate(matrix_list, axis=1)\n\n# # Concatenate matrices vertically (along rows)\n# vertical_concatenated = np.concatenate(matrix_list, axis=0)\n\n# # Print the results\n# print(\"Horizontally concatenated:\")\n# print(horizontal_concatenated)\n\n# print(\"Vertically concatenated:\")\n# print(vertical_concatenated)\n# print(np.concatenate(vertical_concatenated))\n\n# print(matrix_list)\n\n# mfcc_path = \"/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/test-clean/mfcc/\"\n# # transcript_path = \"/kaggle/input/11-785-f23-hw1p2/dev-clean/transcript/84-121123-0001.npy\"\n# mfccnp = []\n# for i in os.listdir(mfcc_path):\n#   mfccnp.append(np.load(mfcc_path+i))\n\n# # transcript_path = np.load(transcript_path)\n# # print(transcript_path.shape)\n# mfccconcat = np.concatenate(mfccnp)\n# mfccpad = np.pad(mfccconcat, ((1, 1), (0, 0)), 'constant', constant_values=0)\n# print(mfccconcat.shape)\n# print(mfccpad.shape)\n# print(mfccpad[1934139])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1gu2TffC5-9w","outputId":"ea5d9ea0-fe19-4128-8ab0-23bf13fb9a56","execution":{"iopub.status.busy":"2023-09-23T19:27:07.578255Z","iopub.execute_input":"2023-09-23T19:27:07.579475Z","iopub.status.idle":"2023-09-23T19:27:07.591053Z","shell.execute_reply.started":"2023-09-23T19:27:07.579442Z","shell.execute_reply":"2023-09-23T19:27:07.589975Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"class AudioDataset(torch.utils.data.Dataset):\n\n    def __init__(self, root, val=True,phonemes=PHONEMES, context=0,\n                 partition=\"train-clean-100\"):  # Feel free to add more arguments\n\n        self.context = context\n        self.phonemes = phonemes\n\n        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n        self.mfcc_dir = root + \"mfcc/\"\n        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n        self.transcript_dir = root + \"transcript/\"\n\n        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n        mfcc_names = sorted(os.listdir(self.mfcc_dir))\n        # TODO: List files in self.transcript_dir using os.listdir in sorted order\n        transcript_names = sorted(os.listdir(self.transcript_dir))\n\n#         c = list(zip(mfcc_names, transcript_names))\n\n#         random.shuffle(c)\n\n#         mfcc_names, transcript_names = zip(*c)\n\n        # Making sure that we have the same no. of mfcc and transcripts\n        assert len(mfcc_names) == len(transcript_names)\n\n        if val:\n            mfcc_names, transcript_names = (mfcc_names[:int(len(mfcc_names))], transcript_names[:int(len(transcript_names))])\n        else:\n            mfcc_names, transcript_names = (mfcc_names[:int(len(mfcc_names) / 10)], transcript_names[:int(len(transcript_names) / 10)])\n        # mfcc_names, transcript_names =  (mfcc_names[20:30],transcript_names[20:30])\n\n        self.mfccs, self.transcripts = [], []\n\n        # TODO: Iterate through mfccs and transcripts\n        for m, t in zip(mfcc_names, transcript_names):\n            #   Load a single mfcc\n            mfcc = np.load(self.mfcc_dir + m) \n            mfcc = (mfcc - np.mean(mfcc, axis=0))/np.std(mfcc, axis=0)\n            #   Do Cepstral Normalization of mfcc (explained in writeup)\n            #   Load the corresponding transcript\n            transcript = np.load(self.transcript_dir + t)\n            transcript = transcript[1:-1]\n            # Remove [SOS] and [EOS] from the transcript\n            # (Is there an efficient way to do this without traversing through the transcript?)\n            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n            #   Append each mfcc to self.mfcc, transcript to self.transcript\n            self.mfccs.append(mfcc)\n            self.transcripts.append(transcript)\n\n        # NOTE:\n        # Each mfcc is of shape T1 x 28, T2 x 28, ...\n        # Each transcript is of shape (T1+2), (T2+2),... before removing [SOS] and [EOS]\n\n        # TODO: Concatenate all mfccs in self.mfccs such that\n        # the final shape is T x 28 (Where T = T1 + T2 + ...)\n        self.mfccs = np.concatenate(self.mfccs, axis=0)\n\n        # TODO: Concatenate all transcripts in self.transcripts such that\n        # the final shape is (T,) meaning, each time step has one phoneme output\n        self.transcripts = np.concatenate(self.transcripts, axis=0)\n        # Hint: Use numpy to concatenate\n\n        # Length of the dataset is now the length of concatenated mfccs/transcripts\n        self.length = len(self.mfccs)\n        # Take some time to think about what we have done.\n        # self.mfcc is an array of the format (Frames x Features).\n        # Our goal is to recognize phonemes of each frame\n        # We can introduce context by padding zeros on top and bottom of self.mfcc\n        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=0)\n\n        # The available phonemes in the transcript are of string data type\n        # But the neural network cannot predict strings as such.\n        # Hence, we map these phonemes to integers\n\n#         print(\"!!\", set(self.transcripts))\n        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n        self.transcripts = [PHONEMES.index(transcript) for transcript in self.transcripts]\n        print(len(self.mfccs), len(self.transcripts))\n\n    #         print(\"!\",self.transcripts)\n    #         print(\"!\",self.transcripts)\n    # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        phonemes = torch.tensor(self.transcripts[ind]).to(device)\n        ind = ind + self.context\n        # print(ind-self.context,ind+self.context+1)\n        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n        frames = self.mfccs[ind - self.context: ind + self.context + 1].flatten()\n\n        frames = torch.FloatTensor(frames).to(device)  # Convert to tensors\n        #         phonemes    = self.transcripts[ind]\n\n        return frames, phonemes\n\n","metadata":{"id":"YpLCvi3AJC5z","execution":{"iopub.status.busy":"2023-09-23T19:27:07.592624Z","iopub.execute_input":"2023-09-23T19:27:07.593345Z","iopub.status.idle":"2023-09-23T19:27:07.617556Z","shell.execute_reply.started":"2023-09-23T19:27:07.593312Z","shell.execute_reply":"2023-09-23T19:27:07.616194Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"class AudioTestDataset(torch.utils.data.Dataset):\n    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n\n        self.context    = context\n        self.phonemes   = phonemes\n\n        self.mfcc_dir       = root+\"mfcc/\"\n        mfcc_names          = os.listdir(self.mfcc_dir)\n\n\n        # Making sure that we have the same no. of mfcc and transcripts\n\n        self.mfccs = []\n\n        # TODO: Iterate through mfccs and transcripts\n        for m in mfcc_names:\n\n        #   Load a single mfcc\n            mfcc        = np.load(self.mfcc_dir+m)\n            self.mfccs.append(mfcc)\n        self.mfccs          = np.concatenate(self.mfccs, axis=0)\n        self.length = len(self.mfccs)\n        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=0)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, ind):\n        ind = ind+self.context\n        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n        frames = self.mfccs[ind-self.context: ind+self.context+1].flatten()\n        frames      = torch.FloatTensor(frames).to(device) # Convert to tensors\n\n        return frames","metadata":{"id":"e8KfVP39S6o7","execution":{"iopub.status.busy":"2023-09-23T19:27:07.623806Z","iopub.execute_input":"2023-09-23T19:27:07.624546Z","iopub.status.idle":"2023-09-23T19:27:07.637377Z","shell.execute_reply.started":"2023-09-23T19:27:07.624511Z","shell.execute_reply":"2023-09-23T19:27:07.636222Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"# Parameters Configuration","metadata":{"id":"qNacQ8bpt9nw"}},{"cell_type":"markdown","source":"Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments.","metadata":{"id":"WE7tsinAuLNy"}},{"cell_type":"code","source":"config = {\n    'epochs'        : 15,\n    'batch_size'    : 1024,\n    'context'       : 35,\n    'init_lr'       : 1e-2,\n    'lr_decay':0.0,\n    'architecture'  : 'very-low-cutoff',\n    'randomSeed':2,\n    'hiddenDims': [2048,1024,1024,1024,1024,512,256]\n    }","metadata":{"id":"PmKwlFqgt_Zq","execution":{"iopub.status.busy":"2023-09-23T19:27:07.638967Z","iopub.execute_input":"2023-09-23T19:27:07.640388Z","iopub.status.idle":"2023-09-23T19:27:07.652184Z","shell.execute_reply.started":"2023-09-23T19:27:07.640345Z","shell.execute_reply":"2023-09-23T19:27:07.650972Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"markdown","source":"# Create Datasets","metadata":{"id":"2mlwaKlDt_2c"}},{"cell_type":"code","source":"#TODO: Create a dataset object using the AudioDataset class for the training data\ntrain_data = AudioDataset(\"/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/train-clean-100/\",context=config['context'], val=False)\n","metadata":{"id":"7xi7V8x8W9z4","execution":{"iopub.status.busy":"2023-09-23T19:27:07.653990Z","iopub.execute_input":"2023-09-23T19:27:07.655109Z","iopub.status.idle":"2023-09-23T19:27:20.775333Z","shell.execute_reply.started":"2023-09-23T19:27:07.655069Z","shell.execute_reply":"2023-09-23T19:27:20.772403Z"},"trusted":true},"execution_count":143,"outputs":[{"name":"stdout","text":"3495464 3495394\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{"id":"Z9SW6hebFDIr"}},{"cell_type":"code","source":"# Define dataloaders for train, val and test datasets\n# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n# We shuffle train dataloader but not val & test dataloader. Why?\n\ntrain_loader = torch.utils.data.DataLoader(\n    dataset     = train_data,\n    num_workers = 0,\n    batch_size  = config['batch_size'],\n    pin_memory  = False,\n    shuffle     = True\n)\n\nprint(\"Batch size     : \", config['batch_size'])\nprint(\"Context        : \", config['context'])\nprint(\"Input size     : \", (2*config['context']+1)*28)\nprint(\"Output symbols : \", len(PHONEMES))\n\nprint(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n# print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n# print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))","metadata":{"id":"4mzoYfTKu14s","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb90c492-df1e-4420-a4e0-20517c1b4f5c","execution":{"iopub.status.busy":"2023-09-23T19:27:20.777205Z","iopub.execute_input":"2023-09-23T19:27:20.777607Z","iopub.status.idle":"2023-09-23T19:27:20.850304Z","shell.execute_reply.started":"2023-09-23T19:27:20.777573Z","shell.execute_reply":"2023-09-23T19:27:20.848639Z"},"trusted":true},"execution_count":144,"outputs":[{"name":"stdout","text":"Batch size     :  1024\nContext        :  35\nInput size     :  1988\nOutput symbols :  42\nTrain dataset samples = 3495394, batches = 3414\n","output_type":"stream"}]},{"cell_type":"code","source":"a,b = train_data.__getitem__(20)\na.shape\n# print(b.is_cuda)\ntorch.set_printoptions(edgeitems=3)\nprint(a.size())\nprint(b)\n# print(a)\ni = 0\nfor i, (a,b) in enumerate(train_data):\n    pass\nprint(i)\nprint(train_data.__len__())\nprint(train_data.__getitem__(12495))\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2vLs6jnBdEvO","outputId":"2c3c3995-4133-4a60-a03b-3e07d9216ae6","execution":{"iopub.status.busy":"2023-09-23T19:27:20.852340Z","iopub.execute_input":"2023-09-23T19:27:20.852878Z","iopub.status.idle":"2023-09-23T19:28:38.817045Z","shell.execute_reply.started":"2023-09-23T19:27:20.852830Z","shell.execute_reply":"2023-09-23T19:28:38.815257Z"},"trusted":true},"execution_count":145,"outputs":[{"name":"stdout","text":"torch.Size([1988])\ntensor(0)\n3495393\n3495394\n(tensor([ 1.4057,  0.1865, -1.9150,  ...,  0.8905, -0.1089,  0.8735]), tensor(21))\n","output_type":"stream"}]},{"cell_type":"code","source":"#Testing code to check if your data loaders are working\nfor i, data in enumerate(train_loader):\n    frames, phoneme = data\n    print(frames.shape, phoneme.shape)\n    break","metadata":{"id":"n-GV3UvgLSoF","execution":{"iopub.status.busy":"2023-09-23T19:28:38.819479Z","iopub.execute_input":"2023-09-23T19:28:38.819904Z","iopub.status.idle":"2023-09-23T19:28:39.591139Z","shell.execute_reply.started":"2023-09-23T19:28:38.819868Z","shell.execute_reply":"2023-09-23T19:28:39.589627Z"},"trusted":true},"execution_count":146,"outputs":[{"name":"stdout","text":"torch.Size([1024, 1988]) torch.Size([1024])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Network Architecture\n","metadata":{"id":"Nxjwve20JRJ2"}},{"cell_type":"markdown","source":"This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline.","metadata":{"id":"3NJzT-mRw6iy"}},{"cell_type":"code","source":"# This architecture will make you cross the very low cutoff\n# However, you need to run a lot of experiments to cross the medium or high cutoff\nclass Network(nn.Module):\n    def __init__(self, sizeList):\n        super(Network, self).__init__()\n        layers = []\n        self.sizeList = sizeList\n         nn.Sequential(\n        nn.Linear(in_features=1988, out_features=2048, bias=True)\n        nn.BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Linear(in_features=2048, out_features=1024, bias=True)\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Linear(in_features=1024, out_features=1024, bias=True)\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Dropout(p=0.2, inplace=False)\n        nn.Linear(in_features=1024, out_features=1024, bias=True)\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Linear(in_features=1024, out_features=1024, bias=True)\n        nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Dropout(p=0.2, inplace=False)\n        nn.Linear(in_features=1024, out_features=512, bias=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Linear(in_features=512, out_features=256, bias=True)\n        nn.LeakyReLU(negative_slope=0.01)\n        nn.Linear(in_features=256, out_features=42, bias=True)\n      )\n        \n    def forward(self, x):\n        return self.net(x)","metadata":{"id":"OvcpontXQq9j","execution":{"iopub.status.busy":"2023-09-23T19:28:39.593608Z","iopub.execute_input":"2023-09-23T19:28:39.594041Z","iopub.status.idle":"2023-09-23T19:28:39.606665Z","shell.execute_reply.started":"2023-09-23T19:28:39.593999Z","shell.execute_reply":"2023-09-23T19:28:39.605030Z"},"trusted":true},"execution_count":147,"outputs":[]},{"cell_type":"markdown","source":"# Define Model, Loss Function and Optimizer","metadata":{"id":"HejoSXe3vMVU"}},{"cell_type":"markdown","source":"Here we define the model, loss function, optimizer and optionally a learning rate scheduler.","metadata":{"id":"xAhGBH7-xxth"}},{"cell_type":"code","source":"INPUT_SIZE  = (2*config['context'] + 1) * 28 # Why is this the case?\nmodel       = Network([INPUT_SIZE] + config[\"hiddenDims\"] + [len(PHONEMES)])\n# frames, p = train_data.__getitem__(0)\nprint(model)\nprint(frames.shape)\n\n# summary(model, frames)\n# Check number of parameters of your network\n# Remember, you are limited to 25 million parameters for HW1 (including ensembles)","metadata":{"id":"_qtrEM1ZvLje","colab":{"base_uri":"https://localhost:8080/","height":463},"outputId":"4f5da38f-355d-4a78-86b5-ad8849e91125","execution":{"iopub.status.busy":"2023-09-23T19:28:39.608470Z","iopub.execute_input":"2023-09-23T19:28:39.608845Z","iopub.status.idle":"2023-09-23T19:28:39.744129Z","shell.execute_reply.started":"2023-09-23T19:28:39.608812Z","shell.execute_reply":"2023-09-23T19:28:39.742777Z"},"trusted":true},"execution_count":148,"outputs":[{"name":"stdout","text":"Network(\n  (net): Sequential(\n    (0): Linear(in_features=1988, out_features=2048, bias=True)\n    (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): LeakyReLU(negative_slope=0.01)\n    (3): Linear(in_features=2048, out_features=1024, bias=True)\n    (4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (5): LeakyReLU(negative_slope=0.01)\n    (6): Linear(in_features=1024, out_features=1024, bias=True)\n    (7): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (8): LeakyReLU(negative_slope=0.01)\n    (9): Dropout(p=0.2, inplace=False)\n    (10): Linear(in_features=1024, out_features=1024, bias=True)\n    (11): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (12): LeakyReLU(negative_slope=0.01)\n    (13): Linear(in_features=1024, out_features=1024, bias=True)\n    (14): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (15): LeakyReLU(negative_slope=0.01)\n    (16): Dropout(p=0.2, inplace=False)\n    (17): Linear(in_features=1024, out_features=512, bias=True)\n    (18): LeakyReLU(negative_slope=0.01)\n    (19): Linear(in_features=512, out_features=256, bias=True)\n    (20): LeakyReLU(negative_slope=0.01)\n    (21): Linear(in_features=256, out_features=42, bias=True)\n  )\n)\ntorch.Size([1024, 1988])\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.cuda.is_available()\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-09-23T19:28:39.746680Z","iopub.execute_input":"2023-09-23T19:28:39.747241Z","iopub.status.idle":"2023-09-23T19:28:39.754751Z","shell.execute_reply.started":"2023-09-23T19:28:39.747195Z","shell.execute_reply":"2023-09-23T19:28:39.753300Z"},"trusted":true},"execution_count":149,"outputs":[]},{"cell_type":"code","source":"criterion = torch.nn.CrossEntropyLoss().to(device) # Defining Loss function.\n# We use CE because the task is multi-class classification\n\noptimizer = torch.optim.AdamW(model.parameters(), lr= config['init_lr']) #Defining Optimizer\n\n# Recommended : Define Scheduler for Learning Rate,\n# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n# You can refer to Pytorch documentation for more information on how to use them.\n\n# Is your training time very high?\n# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html","metadata":{"id":"UROGEVJevKD-","execution":{"iopub.status.busy":"2023-09-23T19:28:39.756484Z","iopub.execute_input":"2023-09-23T19:28:39.757866Z","iopub.status.idle":"2023-09-23T19:28:39.770761Z","shell.execute_reply.started":"2023-09-23T19:28:39.757828Z","shell.execute_reply":"2023-09-23T19:28:39.769709Z"},"trusted":true},"execution_count":150,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training and Validation Functions","metadata":{"id":"IBwunYpyugFg"}},{"cell_type":"markdown","source":"This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs.","metadata":{"id":"1JgeNhx4x2-P"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ngc.collect()","metadata":{"id":"XblOHEVtKab2","colab":{"base_uri":"https://localhost:8080/"},"outputId":"54aebfc5-6131-4c9f-ce21-8bfbaaf9c9c4","execution":{"iopub.status.busy":"2023-09-23T19:28:39.772455Z","iopub.execute_input":"2023-09-23T19:28:39.773988Z","iopub.status.idle":"2023-09-23T19:28:40.327468Z","shell.execute_reply.started":"2023-09-23T19:28:39.773914Z","shell.execute_reply":"2023-09-23T19:28:40.326292Z"},"trusted":true},"execution_count":151,"outputs":[{"execution_count":151,"output_type":"execute_result","data":{"text/plain":"2235"},"metadata":{}}]},{"cell_type":"code","source":"def train(model, train_loader,  optimizer,criterion):\n    model.train()\n    running_loss = 0.0\n    start_time = time.time()\n    dropout = torch.nn.Dropout(p=0.5, inplace=False)\n    acc = 0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        data = data.to(device)\n        target = target\n        \n        outputs = dropout(model(data))\n        loss = criterion(outputs, target)\n        acc += torch.sum(torch.argmax(outputs, dim= 1) == target).item()/outputs.shape[0]\n        running_loss += loss.item()\n        \n        loss.backward()\n        optimizer.step()\n        if batch_idx % 1000 == 0:\n            print(\"Finished \" + str(batch_idx) + \"\\t Timestamp: \"+ str(time.time() - start_time))\n            print(\"loss :\", loss,\"acc :\", acc/(batch_idx+1))\n\n    end_time = time.time()\n    running_loss = running_loss / batch_idx\n    acc = acc/batch_idx\n    print('Training Loss: ', running_loss, 'Time: ',end_time - start_time, 's')\n    return running_loss, acc","metadata":{"id":"8wjPz7DHqKcL","execution":{"iopub.status.busy":"2023-09-23T19:28:40.329167Z","iopub.execute_input":"2023-09-23T19:28:40.330575Z","iopub.status.idle":"2023-09-23T19:28:40.342242Z","shell.execute_reply.started":"2023-09-23T19:28:40.330526Z","shell.execute_reply":"2023-09-23T19:28:40.340880Z"},"trusted":true},"execution_count":152,"outputs":[]},{"cell_type":"code","source":"def test_model(model, test_loader, criterion):\n    with torch.no_grad():\n        model.eval()\n        start_time = time.time()\n        \n        running_loss = 0.0\n        total_predictions = 0.0\n        correct_predictions = 0.0\n        for batch_idx, (data, target) in enumerate(test_loader):\n            data = data.to(device)\n            target = target.to(device)\n            \n            outputs = model(data)\n            \n            _, predicted = torch.max(outputs.data, 1)\n            total_predictions += target.size(0)\n            correct_predictions += (predicted == target).sum().item()\n            \n            loss = criterion(outputs, target).detach()\n            running_loss += loss.item()\n            if batch_idx % 1000 == 0:\n                print(\"Finished \" + str(batch_idx) + \"\\t Timestamp: \"+ str(time.time() - start_time))\n        \n        running_loss /= len(test_loader)\n        acc = (correct_predictions/total_predictions)*100.0\n        print('Testing Loss: ', running_loss)\n        print('Testing Accuracy: ', acc, '%')\n        return running_loss, acc","metadata":{"id":"Q5npQNFH315V","execution":{"iopub.status.busy":"2023-09-23T19:28:40.344155Z","iopub.execute_input":"2023-09-23T19:28:40.345582Z","iopub.status.idle":"2023-09-23T19:28:40.359313Z","shell.execute_reply.started":"2023-09-23T19:28:40.345546Z","shell.execute_reply":"2023-09-23T19:28:40.357927Z"},"trusted":true},"execution_count":153,"outputs":[]},{"cell_type":"markdown","source":"# Weights and Biases Setup","metadata":{"id":"yMd_XxPku5qp"}},{"cell_type":"markdown","source":"This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n\nWe have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning.","metadata":{"id":"tjIbhR1wwbgI"}},{"cell_type":"code","source":"wandb.login(key=\"apikey\") #API Key is in your wandb account, under settings (wandb.ai/settings)","metadata":{"id":"SCDYx5VEu6qI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e094aa8e-f9fc-4f61-b310-5d613c307ad4","execution":{"iopub.status.busy":"2023-09-23T19:28:40.360951Z","iopub.execute_input":"2023-09-23T19:28:40.361321Z","iopub.status.idle":"2023-09-23T19:28:40.628308Z","shell.execute_reply.started":"2023-09-23T19:28:40.361288Z","shell.execute_reply":"2023-09-23T19:28:40.626957Z"},"trusted":true},"execution_count":154,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":154,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Create your wandb run\n# run = wandb.init(\n#     name    = \"first-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n#     reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n#     #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n#     #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n#     project = \"hw1p2\", ### Project should be created in your wandb account\n#     config  = config ### Wandb Config for your run\n# )","metadata":{"id":"xvUnYd3Bw2up","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3e4e1a8f-c0a9-487c-d63e-30fe4f641b51","execution":{"iopub.status.busy":"2023-09-23T19:28:40.630113Z","iopub.execute_input":"2023-09-23T19:28:40.630447Z","iopub.status.idle":"2023-09-23T19:28:40.637306Z","shell.execute_reply.started":"2023-09-23T19:28:40.630418Z","shell.execute_reply":"2023-09-23T19:28:40.635788Z"},"trusted":true},"execution_count":155,"outputs":[]},{"cell_type":"code","source":"### Save your model architecture as a string with str(model)\nmodel_arch  = str(model)\n\n### Save it in a txt file\narch_file   = open(\"model_arch.txt\", \"w\")\nfile_write  = arch_file.write(model_arch)\narch_file.close()\n\n### log it in your wandb run with wandb.save()\n# wandb.save('model_arch.txt')","metadata":{"id":"wft15E_IxYFi","colab":{"base_uri":"https://localhost:8080/"},"outputId":"45b5707a-fb1c-43b5-8ebd-76897f84c0da","execution":{"iopub.status.busy":"2023-09-23T19:28:40.639512Z","iopub.execute_input":"2023-09-23T19:28:40.641024Z","iopub.status.idle":"2023-09-23T19:28:40.651658Z","shell.execute_reply.started":"2023-09-23T19:28:40.640975Z","shell.execute_reply":"2023-09-23T19:28:40.650727Z"},"trusted":true},"execution_count":156,"outputs":[]},{"cell_type":"markdown","source":"# Experiment","metadata":{"id":"nclx_04fu7Dd"}},{"cell_type":"markdown","source":"Now, it is time to finally run your ablations! Have fun!","metadata":{"id":"MdLMWfEpyGOB"}},{"cell_type":"code","source":"# Iterate over number of epochs to train and evaluate your model\ntorch.cuda.empty_cache()\ngc.collect()\n# wandb.watch(model, log=\"all\")\n\n# TODO: Create a dataset object using the AudioDataset class for the validation data\nval_data= AudioDataset(\"/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/dev-clean/\",context=config['context'], val=True)\n\nval_loader = torch.utils.data.DataLoader(\n    dataset     = val_data,\n    num_workers = 0,\n    batch_size  = config['batch_size'],\n    pin_memory  = False,\n    shuffle     = False\n)\n\nfor epoch in range(config['epochs']):\n    \n\n    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n#     print(model,train_loader,optimizer, criterion, sep=\"###\")\n    train_loss, train_acc   = train(model, train_loader, optimizer, criterion)\n    \n    \n#     model.share_memory()\n#     processes = []\n#     for rank in range(num_processes):\n#         p = mp.Process(target=train, args=(model,))\n#         p.start()\n#         processes.append(p)\n#     for p in processes:\n#         p.join()\n\n\n#     mp.set_start_method('spawn')\n#     p = mp.Process(target=train, args=(model,train_loader,optimizer,criterion), daemon=True)\n#     p.start()\n#     p.join()\n#     print(\"x =\", x.item())\n    \n    \n    val_loss, val_acc       = test_model(model, val_loader, criterion)\n\n    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n\n    ### Log metrics at each epoch in your run\n    # Optionally, you can log at each batch inside train/eval functions\n    # (explore wandb documentation/wandb recitation)\n#     wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n#                'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n\n    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\ntorch.save(model.state_dict(), './model.model')\nmodel.save(os.path.join(wandb.run.dir, \"model.h5\"))\n### Finish your wandb run\nrun.finish()","metadata":{"id":"MG4F77Nm0Am9","scrolled":true,"execution":{"iopub.status.busy":"2023-09-23T19:28:40.653154Z","iopub.execute_input":"2023-09-23T19:28:40.653605Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"1928274 1928204\n\nEpoch 1/15\nFinished 0\t Timestamp: 1.3941848278045654\nloss : tensor(3.7473, grad_fn=<NllLossBackward0>) acc : 0.0224609375\nFinished 1000\t Timestamp: 568.8796234130859\nloss : tensor(2.5314, grad_fn=<NllLossBackward0>) acc : 0.33882718843656345\nFinished 2000\t Timestamp: 1136.3880393505096\nloss : tensor(2.4039, grad_fn=<NllLossBackward0>) acc : 0.3755744198213393\nFinished 3000\t Timestamp: 1703.9138746261597\nloss : tensor(2.3386, grad_fn=<NllLossBackward0>) acc : 0.3980535134121959\nTraining Loss:  2.544311793710422 Time:  1938.8547694683075 s\nFinished 0\t Timestamp: 0.21741747856140137\nFinished 1000\t Timestamp: 219.51888918876648\nTesting Loss:  1.4894773649435387\nTesting Accuracy:  61.00962346307755 %\n\tTrain Acc 40.4991%\tTrain Loss 2.5443\t Learning Rate 0.0100000\n\tVal Acc 6100.9623%\tVal Loss 1.4895\n\nEpoch 2/15\nFinished 0\t Timestamp: 1.1274447441101074\nloss : tensor(2.3310, grad_fn=<NllLossBackward0>) acc : 0.44921875\nFinished 1000\t Timestamp: 567.7061786651611\nloss : tensor(2.2361, grad_fn=<NllLossBackward0>) acc : 0.46461741383616384\nFinished 2000\t Timestamp: 1142.3995952606201\nloss : tensor(2.1930, grad_fn=<NllLossBackward0>) acc : 0.46721609898175914\nFinished 3000\t Timestamp: 1718.4037790298462\nloss : tensor(2.1542, grad_fn=<NllLossBackward0>) acc : 0.47041611129623456\nTraining Loss:  2.2505187163281715 Time:  1957.76900100708 s\nFinished 0\t Timestamp: 0.2239232063293457\nFinished 1000\t Timestamp: 223.97997426986694\nTesting Loss:  1.3583807643431767\nTesting Accuracy:  64.36870787530779 %\n\tTrain Acc 47.2090%\tTrain Loss 2.2505\t Learning Rate 0.0100000\n\tVal Acc 6436.8708%\tVal Loss 1.3584\n\nEpoch 3/15\nFinished 0\t Timestamp: 1.0961532592773438\nloss : tensor(2.1331, grad_fn=<NllLossBackward0>) acc : 0.48046875\nFinished 1000\t Timestamp: 587.9056842327118\nloss : tensor(2.2067, grad_fn=<NllLossBackward0>) acc : 0.48948219749001\nFinished 2000\t Timestamp: 1174.119208574295\nloss : tensor(2.1766, grad_fn=<NllLossBackward0>) acc : 0.49065311094452774\nFinished 3000\t Timestamp: 1752.3191781044006\nloss : tensor(2.1574, grad_fn=<NllLossBackward0>) acc : 0.49159915444851715\nTraining Loss:  2.1519896383687893 Time:  1993.2777335643768 s\nFinished 0\t Timestamp: 0.21344375610351562\nFinished 1000\t Timestamp: 222.14024448394775\nTesting Loss:  1.3140981223616124\nTesting Accuracy:  65.24900892229245 %\n\tTrain Acc 49.2317%\tTrain Loss 2.1520\t Learning Rate 0.0100000\n\tVal Acc 6524.9009%\tVal Loss 1.3141\n\nEpoch 4/15\nFinished 0\t Timestamp: 1.1990082263946533\nloss : tensor(2.1629, grad_fn=<NllLossBackward0>) acc : 0.48828125\nFinished 1000\t Timestamp: 601.9305090904236\nloss : tensor(2.0802, grad_fn=<NllLossBackward0>) acc : 0.4995600103021978\nFinished 2000\t Timestamp: 1199.2661907672882\nloss : tensor(2.0436, grad_fn=<NllLossBackward0>) acc : 0.5010341508933034\nFinished 3000\t Timestamp: 1799.7349517345428\nloss : tensor(2.0697, grad_fn=<NllLossBackward0>) acc : 0.5015164216094635\nTraining Loss:  2.104505783715906 Time:  2048.2304413318634 s\nFinished 0\t Timestamp: 0.21537375450134277\nFinished 1000\t Timestamp: 221.16768956184387\nTesting Loss:  1.2545281877373433\nTesting Accuracy:  65.72344005094897 %\n\tTrain Acc 50.1994%\tTrain Loss 2.1045\t Learning Rate 0.0100000\n\tVal Acc 6572.3440%\tVal Loss 1.2545\n\nEpoch 5/15\nFinished 0\t Timestamp: 1.1847758293151855\nloss : tensor(2.0750, grad_fn=<NllLossBackward0>) acc : 0.5166015625\nFinished 1000\t Timestamp: 605.0462319850922\nloss : tensor(2.0637, grad_fn=<NllLossBackward0>) acc : 0.5099490353396603\nFinished 2000\t Timestamp: 1212.475516796112\nloss : tensor(2.1167, grad_fn=<NllLossBackward0>) acc : 0.5020068090954523\nFinished 3000\t Timestamp: 1817.181275844574\nloss : tensor(2.0929, grad_fn=<NllLossBackward0>) acc : 0.5036276970176607\nTraining Loss:  2.098209489310483 Time:  2059.4983882904053 s\nFinished 0\t Timestamp: 0.21559786796569824\nFinished 1000\t Timestamp: 221.66185855865479\nTesting Loss:  1.276347590226783\nTesting Accuracy:  66.51459077981376 %\n\tTrain Acc 50.4489%\tTrain Loss 2.0982\t Learning Rate 0.0100000\n\tVal Acc 6651.4591%\tVal Loss 1.2763\n\nEpoch 6/15\nFinished 0\t Timestamp: 1.174851894378662\nloss : tensor(2.0689, grad_fn=<NllLossBackward0>) acc : 0.509765625\nFinished 1000\t Timestamp: 597.1472835540771\nloss : tensor(2.2236, grad_fn=<NllLossBackward0>) acc : 0.5040945382742258\nFinished 2000\t Timestamp: 1174.7189700603485\nloss : tensor(2.1420, grad_fn=<NllLossBackward0>) acc : 0.5058544946276862\nFinished 3000\t Timestamp: 1755.1299352645874\nloss : tensor(1.9661, grad_fn=<NllLossBackward0>) acc : 0.5078974326266245\nTraining Loss:  2.0818136682127566 Time:  1992.0108125209808 s\nFinished 0\t Timestamp: 0.3307507038116455\nFinished 1000\t Timestamp: 223.1372630596161\nTesting Loss:  1.2396844813082122\nTesting Accuracy:  66.15601876150033 %\n\tTrain Acc 50.8423%\tTrain Loss 2.0818\t Learning Rate 0.0100000\n\tVal Acc 6615.6019%\tVal Loss 1.2397\n\nEpoch 7/15\nFinished 0\t Timestamp: 1.2157604694366455\nloss : tensor(2.0105, grad_fn=<NllLossBackward0>) acc : 0.5146484375\nFinished 1000\t Timestamp: 574.4948627948761\nloss : tensor(2.0083, grad_fn=<NllLossBackward0>) acc : 0.5154045173576424\nFinished 2000\t Timestamp: 1145.8988559246063\nloss : tensor(2.0562, grad_fn=<NllLossBackward0>) acc : 0.5150657093328336\nFinished 3000\t Timestamp: 1718.8086516857147\nloss : tensor(1.9629, grad_fn=<NllLossBackward0>) acc : 0.5146347701807731\nTraining Loss:  2.0415963987922723 Time:  1956.3787069320679 s\nFinished 0\t Timestamp: 0.2214672565460205\nFinished 1000\t Timestamp: 220.34878754615784\nTesting Loss:  1.2813446457851718\nTesting Accuracy:  65.99255058074768 %\n\tTrain Acc 51.4645%\tTrain Loss 2.0416\t Learning Rate 0.0100000\n\tVal Acc 6599.2551%\tVal Loss 1.2813\n\nEpoch 8/15\nFinished 0\t Timestamp: 1.1357426643371582\nloss : tensor(2.0784, grad_fn=<NllLossBackward0>) acc : 0.49609375\nFinished 1000\t Timestamp: 578.0252814292908\nloss : tensor(2.0809, grad_fn=<NllLossBackward0>) acc : 0.5174054461163836\nFinished 2000\t Timestamp: 1157.2468087673187\nloss : tensor(2.1079, grad_fn=<NllLossBackward0>) acc : 0.5171628053160919\nFinished 3000\t Timestamp: 1738.3610305786133\nloss : tensor(2.0176, grad_fn=<NllLossBackward0>) acc : 0.5168319544526825\nTraining Loss:  2.0314345733531924 Time:  1983.9398217201233 s\nFinished 0\t Timestamp: 0.21324777603149414\nFinished 1000\t Timestamp: 220.36563897132874\nTesting Loss:  1.5708970383062737\nTesting Accuracy:  65.36632016114477 %\n\tTrain Acc 51.6873%\tTrain Loss 2.0314\t Learning Rate 0.0100000\n\tVal Acc 6536.6320%\tVal Loss 1.5709\n\nEpoch 9/15\nFinished 0\t Timestamp: 1.1744742393493652\nloss : tensor(2.0429, grad_fn=<NllLossBackward0>) acc : 0.5205078125\nFinished 1000\t Timestamp: 584.3826584815979\nloss : tensor(1.9524, grad_fn=<NllLossBackward0>) acc : 0.5119138673826173\nFinished 2000\t Timestamp: 1173.3266370296478\nloss : tensor(2.0570, grad_fn=<NllLossBackward0>) acc : 0.5155703398300849\nFinished 3000\t Timestamp: 1759.6676177978516\nloss : tensor(2.0112, grad_fn=<NllLossBackward0>) acc : 0.5163291923525491\nTraining Loss:  2.0331421424104015 Time:  2003.6700329780579 s\nFinished 0\t Timestamp: 0.21547532081604004\nFinished 1000\t Timestamp: 221.00419974327087\nTesting Loss:  1.241431464023904\nTesting Accuracy:  66.53129025766982 %\n\tTrain Acc 51.6475%\tTrain Loss 2.0331\t Learning Rate 0.0100000\n\tVal Acc 6653.1290%\tVal Loss 1.2414\n\nEpoch 10/15\nFinished 0\t Timestamp: 1.0915558338165283\nloss : tensor(1.9780, grad_fn=<NllLossBackward0>) acc : 0.5068359375\nFinished 1000\t Timestamp: 595.7445168495178\nloss : tensor(1.9569, grad_fn=<NllLossBackward0>) acc : 0.5222424060314685\nFinished 2000\t Timestamp: 1177.9289801120758\nloss : tensor(2.0595, grad_fn=<NllLossBackward0>) acc : 0.5191774230072463\nFinished 3000\t Timestamp: 1759.8203449249268\nloss : tensor(2.0258, grad_fn=<NllLossBackward0>) acc : 0.5186223732714095\nTraining Loss:  2.022345063241388 Time:  2000.784998178482 s\nFinished 0\t Timestamp: 0.21592974662780762\nFinished 1000\t Timestamp: 219.71111726760864\nTesting Loss:  1.253387664375538\nTesting Accuracy:  66.6543062870941 %\n\tTrain Acc 51.8792%\tTrain Loss 2.0223\t Learning Rate 0.0100000\n\tVal Acc 6665.4306%\tVal Loss 1.2534\n\nEpoch 11/15\nFinished 0\t Timestamp: 1.1791017055511475\nloss : tensor(1.9811, grad_fn=<NllLossBackward0>) acc : 0.513671875\nFinished 1000\t Timestamp: 590.723655462265\nloss : tensor(1.9567, grad_fn=<NllLossBackward0>) acc : 0.5228940980894106\nFinished 2000\t Timestamp: 1179.7965936660767\nloss : tensor(2.0578, grad_fn=<NllLossBackward0>) acc : 0.5208801849075462\nFinished 3000\t Timestamp: 1774.214685678482\nloss : tensor(2.0081, grad_fn=<NllLossBackward0>) acc : 0.5205406791486171\nTraining Loss:  2.0125073489994727 Time:  2021.2601156234741 s\nFinished 0\t Timestamp: 0.21650910377502441\nFinished 1000\t Timestamp: 222.32129549980164\nTesting Loss:  1.242793932778567\nTesting Accuracy:  66.42248434294297 %\n\tTrain Acc 52.0624%\tTrain Loss 2.0125\t Learning Rate 0.0100000\n\tVal Acc 6642.2484%\tVal Loss 1.2428\n\nEpoch 12/15\nFinished 0\t Timestamp: 1.0650982856750488\nloss : tensor(1.9117, grad_fn=<NllLossBackward0>) acc : 0.53125\nFinished 1000\t Timestamp: 590.0993759632111\nloss : tensor(1.9291, grad_fn=<NllLossBackward0>) acc : 0.5229497065434565\nFinished 2000\t Timestamp: 1172.4340415000916\nloss : tensor(2.0896, grad_fn=<NllLossBackward0>) acc : 0.5204370471014492\nFinished 3000\t Timestamp: 1761.515707731247\nloss : tensor(2.0136, grad_fn=<NllLossBackward0>) acc : 0.5198390900949683\nTraining Loss:  2.015526373261341 Time:  2005.3169434070587 s\nFinished 0\t Timestamp: 0.21595215797424316\nFinished 1000\t Timestamp: 221.0666720867157\nTesting Loss:  1.2381169258156146\nTesting Accuracy:  66.43197503998539 %\n\tTrain Acc 52.0064%\tTrain Loss 2.0155\t Learning Rate 0.0100000\n\tVal Acc 6643.1975%\tVal Loss 1.2381\n\nEpoch 13/15\nFinished 0\t Timestamp: 1.229738473892212\nloss : tensor(1.9722, grad_fn=<NllLossBackward0>) acc : 0.5322265625\nFinished 1000\t Timestamp: 589.8540711402893\nloss : tensor(1.9993, grad_fn=<NllLossBackward0>) acc : 0.5247516155719281\nFinished 2000\t Timestamp: 1171.5443794727325\nloss : tensor(1.9978, grad_fn=<NllLossBackward0>) acc : 0.5235765906109445\nFinished 3000\t Timestamp: 1755.9769260883331\nloss : tensor(2.0284, grad_fn=<NllLossBackward0>) acc : 0.5229874547025991\nTraining Loss:  2.0063967125666586 Time:  1995.8587429523468 s\nFinished 0\t Timestamp: 0.2134084701538086\nFinished 1000\t Timestamp: 219.3607635498047\nTesting Loss:  1.2657572197116864\nTesting Accuracy:  66.16265706325679 %\n\tTrain Acc 52.2145%\tTrain Loss 2.0064\t Learning Rate 0.0100000\n\tVal Acc 6616.2657%\tVal Loss 1.2658\n\nEpoch 14/15\nFinished 0\t Timestamp: 1.0485188961029053\nloss : tensor(2.0301, grad_fn=<NllLossBackward0>) acc : 0.5341796875\nFinished 1000\t Timestamp: 578.0994279384613\nloss : tensor(2.0114, grad_fn=<NllLossBackward0>) acc : 0.525278432504995\nFinished 2000\t Timestamp: 1155.627049446106\nloss : tensor(1.9988, grad_fn=<NllLossBackward0>) acc : 0.5224794829147926\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Testing and submission to Kaggle","metadata":{"id":"_kXwf5YUo_4A"}},{"cell_type":"markdown","source":"Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function.","metadata":{"id":"WI1hSFYLpJvH"}},{"cell_type":"markdown","source":"","metadata":{"id":"H-VbSWAeCMEO"}},{"cell_type":"code","source":"def test(model, test_loader):\n    ### What you call for model to perform inference?\n    model.eval()  # TODO train or eval?\n\n    ### List to store predicted phonemes of test data\n    test_predictions = []\n    predicted_phoneme = None\n\n    ### Which mode do you need to avoid gradients?\n    with torch.inference_mode():  # TODO\n\n        for i, mfccs in enumerate(test_loader):\n            mfccs = mfccs.to(device)\n\n            logits = model(mfccs)\n            # print(logits.shape)\n            l = torch.argmax(logits, dim=1)\n            # print(\"!!!\",len(l))\n            # print(\"!!!\",l)\n            ### Get most likely predicted phoneme with argmax\n            predicted_phoneme = [PHONEMES[int(i)] for i in l]\n            # print(predicted_phoneme)\n            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n            # TODO\n            test_predictions.extend(predicted_phoneme)\n            # print(\"!!\",PHONEMES[predicted_phonemes])\n\n\n    return test_predictions","metadata":{"id":"R-SU9fZ3xHtk","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(model)","metadata":{"execution":{"iopub.status.busy":"2023-09-24T04:47:31.149662Z","iopub.execute_input":"2023-09-24T04:47:31.150547Z","iopub.status.idle":"2023-09-24T04:47:31.599814Z","shell.execute_reply.started":"2023-09-24T04:47:31.150499Z","shell.execute_reply":"2023-09-24T04:47:31.597961Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m)\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"],"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# model       = Network(INPUT_SIZE, len(train_data.phonemes)).to(device)\nmodel.load_state_dict(torch.load(\"./model.model\"))\n\n# TODO: Create a dataset object using the AudioTestDataset class for the test data\ntest_data = AudioTestDataset(\"/kaggle/input/11785-hw1p2-f23/11-785-f23-hw1p2/test-clean/\", context=config['context'])\ntest_loader = torch.utils.data.DataLoader(\n    dataset     = test_data,\n    num_workers = 0,\n    batch_size  = config['batch_size'],\n    pin_memory  = True,\n    shuffle     = False\n)\n\npredictions = test(model, test_loader)\nprint(test_loader.__len__())\n# predicted_phoneme = []\n# print(test_data.__getitem__(1888))\n\n# # for i in range(0,1887):\n# #   test_loader.__next__()\n\n# # print(test_loader.__next__())\n\n# # while True:\n# #     (x,y) = next(test_loader)\n# #     print(y.shape)\n\n# # for i in test_loader:\n# #   print(i)\n\n\n# for mfccs in test_loader:\n#   print(mfccs.shape)\n#   mfccs = mfccs.to(device)\n#   logits = model(mfccs)\n#   print(logits.shape)\n#   ## Get most likely predicted phoneme with argmax\n#   predicted_phoneme.append(torch.argmax(torch.argmax(logits, dim=1)))\n#   break\n\n# print(predicted_phoneme)\n# # print(i,mfccs)\n# print(mfccs.shape)\n\n","metadata":{"id":"wG9v6Xmxu7wp","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2f64d934-5cc7-4f62-f952-2ef2f4ebd065","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.__getitem__(250))\n# print(predictions)\nprint(len(predictions))\n# print(len(list(predictions)))\n# print(torch.argmax(predictions))\n# print(PHONEMES[torch.argmax(predictions)])\n# print(PHONEM)","metadata":{"id":"TAIHWSY8MhUT","colab":{"base_uri":"https://localhost:8080/"},"outputId":"bc155738-fce1-4163-b0b3-2a5916b228cd","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Create CSV file with predictions\nwith open(\"./submission.csv\", \"w+\") as f:\n    f.write(\"id,label\\n\")\n    for i in range(len(predictions)):\n        f.write(\"{},{}\\n\".format(i, predictions[i]))","metadata":{"id":"ZE1hRnvf0bFz","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pwd","metadata":{"id":"0SLhiFRuBjlD","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Submit to kaggle competition using kaggle API (Uncomment below to use)\n!kaggle competitions submit -c 11785-hw1p2-f23 -f ./submission.csv -m \"Test Submission\"\n\n### However, its always safer to download the csv file and then upload to kaggle","metadata":{"id":"LjcammuCxMKN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}